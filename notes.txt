- The HRDPS model runs four times per day at 00Z, 06Z, 12Z, 18Z
- We focus on serving west coast pilots who want a morning forecast before they head out and an evening forecast before bedtime
- This is well served by the 06Z and 18Z runs
- Each run takes 7 to 9 hours to complete, so the data for the 06Z run is available at 12:30Z (4:30 AM PT) and the data for the 18Z run is available at 01:15Z (5:15 PM PT)
- This page that describes the model is outdated: https://www.weather.gc.ca/grib/grib2_HRDPS_HR_e.html
- This page is up to date: https://eccc-msc.github.io/open-data/msc-data/nwp_hrdps/readme_hrdps-datamart_en/
- ECCC went from using polar-stereographic grids to using rotated lan-lon grids.

- canadarasp/crontab calls canadarasp/aws-utils/download-data-and-start-server.sh 3 times a day
	- 01:15Z for HRDPS -> Process data from the 18Z run from the previous day
	- 08:15Z for GDPS
	- 12:30Z for HRDPS -> Process data from the 06Z run
	- The Z means UTC
	- These times are chosen so that the ECCC models are finished by then
	- We download 48 hours for the HRDPS (it outputs data every hour) and 99 hours for the GDPS

download-data-and-start-server.sh hrdps

- canadarasp/aws-utils/download-data-and-start-server.sh calls canadarasp/continental-test/guess-time.sh

./guess-time.sh $MODEL

- That script calculates HOUR, DAY, MONTH and YEAR values to know which HRDPS run to download

- canadarasp/aws-utils/download-data-and-start-server.sh then calls canadarasp/continental-test/download-data.sh

- That scripts calls guess-time.sh and then it calls canadarasp/continental-test/model-parameters.sh

model-parameters.sh hrdps

- This is where things get weird. The HRDPS DIRECTORY parameter doesn't exist. In other words, this URL doesn't exist: https://dd.weather.gc.ca/model_hrdps/continental/grib2/
- Only the HRDPS ROT directory exists: https://dd.weather.gc.ca/model_hrdps/continental/2.5km/
- But since $MODEL is equal to "hrpds" starting at the crontab, we are in theory trying to access the URL that doesn't exist.
- This doesn't make sense to me. Maybe Andrew hasn't pushed his changes to the crontab? I'm not sure.
- Let's assume that $MODEL is equal to "hrdps_rot" for now.
- These are the variables that get set in model-parameters.sh:

export MODEL=$MODEL
export FILETOPROBE="PRES_SFC_0" # how to tell if data is good at last hour

export WEBSERVER="dd.weather.gc.ca"
export DIRECTORY="model_hrdps/continental/2.5km"
export FILEHEADER="$YEAR$MONTH$DAY"T"$HOUR""Z_MSC_HRDPS"
export TIMESTART="1" # no prate data for zero
export TIMESTEP="1"
export TIMESTOP="48"
export RESOLUTION=""
export TAIL=".grib2"
export FILE="HRDPS-ROT-files.txt"
export OUTPUTDIR="/mnt/input/hrdps"
export TILEDIR="/mnt/windgram-tiles/hrdps"
export PNGDIR="/mnt/map-pngs/hrdps"
export XMIN=-152
export XMAX=-42
export XSTEP=2
export YMIN=26
export YSTEP=2
export YMAX=70
export FILETOPROBE="PRES_Sfc" # how to tell if data is good at last hour

filename () {
 FILELABEL=$1;
 H=$2;
 echo $FILEHEADER"_$FILELABEL"$RESOLUTION$YEAR$MONTH$DAY$HOUR"_PT0"$H"H"$TAIL
}
downloadfilename () {
 FILELABEL=$1;
 H=$2;
 echo $FILEHEADER"_$FILELABEL""_RLatLon0.0225_PT0"$H"H"$TAIL
}

export XVALS=($(seq $XMIN $XSTEP $XMAX))
export YVALS=($(seq $YMIN $YSTEP $YMAX))

export TIMES=($(seq -w $TIMESTART $TIMESTEP $TIMESTOP))
export LEVELS=(550 600 650 700 750 800 850 875 900 925 950 970 985 1000 1015)

- download-data.sh then downloads all the files listed in /tmp/wget.jobs
- Finally, it calls canadarasp/continental-test/clean-up-hgt-sfc.sh and canadarasp/continental-test/rename-hrdps-rot.sh
- Those scripts create symbolic links for missing files and rename HRDPS ROT files to HRDPS files so that the data processing works properly

- The compute server is then started
- The compute server when started starts up continental-test/do-rasp-run-continental.sh through /etc/rc.local
- canadarasp/config-files/compute-rc.local has this call in it:

su ubuntu -c '(cd /home/ubuntu/continental-test ; mv rasp-run.log rasp-run.log.old ; ./do-rasp-run-continental.sh >& rasp-run.log &)'

canadarasp/continental-test/do-rasp-run-continental.sh

- This script initializes the internal SSD and creates a local swap file (needed for stability later, though we don't actually swap in steady state).
- This script attaches and mounts the download box, copies the data to the local SSD, detaches and deletes the download box.
- Then it calls continental-test/do-hrdps-plots-continental.sh

do-hrdps-plots-continental.sh $YEAR $MONTH $DAY $HOUR

- Generate windgrams, map tiles, and dynamic windgram data tiles and upload them all to the web-server
- YEAR MONTH DAY HOUR are the forecast initialization time. These are just handled by guess-time.sh MODEL. This works fine, though the design is relatively fragile. We could frob this from the filenames on the download box. But this works OK for now.

There are six main steps here:

1) do-generate-new-variables.sh which computes some new parameters (HCRIT, WSTAR) across the whole domain and are used in the map soaring forecasts. These are re-computed in the windgram scripts because I was too lazy to fix them.

2) clip-wind-to-terrain.sh which clips the wind fields to terrain (this seems to be broken --- this has to do with which file has the terrain information). This makes a nicer looking map, but isn't truly necessary.

3) do-tile-generation.sh which generates the grib2 files used by the windgram generation scripts (both static and dynamic generation).

4) do-windgrams-continental.sh which generates the static windgrams for all the sites in locations.txt and uploads them back to the web server (along with a locations / regions list to be read by the javascript front end). It also uploads the grib2 tile files generated in step 3 back to the web server for the dynamic windgram generation.

5) generate-single-hrdps-plot-continental.lisp which generates the PNG and pseudo-PNG files for the google maps interface.

6) upload-map-pngs.sh uploads the PNG files back to the web server.

- Generally for each of these we grab the model-parameters which are encoded in model-parameters.sh and model-parameters.lisp to tell us something about the files we are processing.

